{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "import glob\n",
    "PDF_data = []\n",
    "pdfs = glob.glob(\"pdf/*.pdf\")\n",
    "\n",
    "for pdfFile in glob.glob(\"pdf/*.pdf\"):\n",
    "    loader = PyMuPDFLoader(pdfFile)\n",
    "    PDF_data.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=5)\n",
    "all_splits = text_splitter.split_documents(PDF_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# model_name = \"GanymedeNil/text2vec-base-chinese\"\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cuda:0'}\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name,\n",
    "                                  model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 儲存向量資料庫\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'db'\n",
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embedding, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 載入過去的創建的資料庫\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'db'\n",
    "vectordb = Chroma(embedding_function=embedding, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "# model_path = \"./model/Taiwan-LLM-7B-v2.1-chat-Q4_K_M.gguf\"\n",
    "model_path = \"./model/chinese-alpaca-2-7b.Q4_K_M.gguf\"\n",
    "from queue import Queue, Empty\n",
    "from typing import Any\n",
    "from threading import Thread\n",
    "\n",
    "q = Queue()\n",
    "job_done = object()\n",
    "\n",
    "class QueueCallback(BaseCallbackHandler):\n",
    "    \"\"\"Callback handler for streaming LLM responses to a queue.\"\"\"\n",
    "\n",
    "    def __init__(self, q):\n",
    "        self.q = q\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        self.q.put(token)\n",
    "\n",
    "    def on_llm_end(self, *args, **kwargs: Any) -> None:\n",
    "        return self.q.empty()\n",
    "callbacks = [QueueCallback(q)]\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=10,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    # callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    callbacks=callbacks,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template='<<SYS>> \\n 你是國資圖的導覽員，負責回答大家的問題  \\n <</SYS>> \\n\\n [INST] 請使用中文回覆: \\n\\n {question} [/INST]')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"<<SYS>> \\n 你是一名導覽員，負責回答大家的問題 \\\n",
    " \\n <</SYS>> \\n\\n [INST] 請使用中文回覆: \\n\\n {question} [/INST]\"\"\",\n",
    ")\n",
    "\n",
    "DEFAULT_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"你是一名導覽員，負責回答大家的問題 \\\n",
    " 請使用中文回覆: {question}\"\"\",\n",
    ")\n",
    "\n",
    "QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\n",
    "    default_prompt=DEFAULT_SEARCH_PROMPT,\n",
    "    conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],\n",
    ")\n",
    "\n",
    "prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  你好\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def answer(question):\n",
    "    def task():\n",
    "        response = qa.invoke(question)\n",
    "        q.put(job_done)\n",
    "    \n",
    "    t = Thread(target=task)\n",
    "    t.start()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def res(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        question = history[-1][0]\n",
    "        print(\"Question: \", question)\n",
    "        history[-1][1] = \"\"\n",
    "        answer(question=question)\n",
    "        while True:\n",
    "          try:\n",
    "            next_token = q.get(True, timeout=1)\n",
    "            if next_token is job_done:\n",
    "              break\n",
    "            history[-1][1] += next_token\n",
    "            yield history\n",
    "          except Empty:\n",
    "            continue\n",
    "\n",
    "    msg.submit(res, [msg, chatbot], [msg, chatbot], queue=False).then(bot, chatbot, chatbot)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
